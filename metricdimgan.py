# -*- coding: utf-8 -*-
"""MetricDimGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mUTsU5kfGqqMB4LYk7NogV-5JpXaYbbh
"""

import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as tfl
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Flatten

import networkx as nx

#@title ICH Algorithm (Hidden) { display-mode: "form" }
# ICH Algorithm by Richard Tillquist

from scipy.stats import entropy
from scipy.special import comb
from itertools import product, combinations, islice, repeat
import multiprocessing as mp
import pickle

########################
### READ/WRITE STATE ###
########################
#Save a snapshot of the algorithm state as a list of tuples (tags, chosen elements) using Python pickle
#input: tags - a dictionary of tags for each element based on the chosen columns
#       chosen - a list of the columns chosen so far
#       saveFile - name of the file to write to
#       overwrite - if true overwrite the contents of the file, otherwise append, default is False
def saveState(tags, chosen, saveFile, overwrite=False):
  L = [] if overwrite else readState(saveFile)
  L.append((tags, chosen))
  with open(saveFile, 'wb') as o:
    pickle.dump(L, o, protocol=pickle.HIGHEST_PROTOCOL)

#Read a list of saved states of the algorithm
#input: inFile - the file to read
#return: a list of (tag, chosen) pairs
def readState(inFile):
  L = []
  with open(inFile, 'rb') as f:
    L = pickle.load(f)
  return L

#####################
### ICH Algorithm ###
#####################
#Apply the ICH algorithm to approximate metric dimension or multilateration
#input: M - a list of lists, dictionary of dictionaries, or networkx graph object on which to perform the ICH algorithm
#           if a list of lists, the ICH algorithm will be applied directly
#           if a dictionary of dictionaries, the colNames argument will be used as a complete list of columns. if this is empty, an arbitrary key list from an elemt of M will be used
#           if a networkx graph object, the distance matrix will be used
#       colNames - optional list of columns. if M is a dictionary of dictionaries and colNames is not empty it will be used as the set of columns to consider, defaults to empty
#       dictDefault - optional default value to use if M is a dictionary of dictionaries, defaults to -1
#       useFullDistance - optional boolean value. if true and M is a graph, convert M to a list of lists before continuing. note that distances in the graph are assumed to be positive.
#                         if false, individual columns of a distance matrix are generated on demand in the colEntropy function
#       name - optional prefix to give to a file in which to save current state of the algorithm, defaults to empty
#       stateFile - optional name of a file to read current state from, default is empty
#       randOrder - optional boolean value. if true randomize the order in which columns are checked, defaults to true
#       procs - optional number of processes to run, defaults to 1
#return: a list of chosen columns representing a resolving set
def ich(M, colNames=[], dictDefault=-1, useFullDistance=True, name='', stateFile='', randOrder=True, procs=1):
  progressFile = name+'_ich_progress'
  if name: saveState({}, [], progressFile, overwrite=True)
  if isinstance(M, nx.classes.graph.Graph) and useFullDistance:
    nodes = sorted(M.nodes())
    M = nx.floyd_warshall(M)
    M = [[int(M[u][v]) if (v in M and M[u][v]!=np.inf) else -1 for v in nodes] for u in nodes]
  distr = {}
  tags = {}
  chosen = []
  if stateFile:
    (tags, chosen) = readState(stateFile)[-1]
    for t in tags:
      if tags[t] not in distr: distr[tags[t]] = []
      distr[tags[t]].append(t)
  elif isinstance(M, list): tags = {i:'' for i in range(len(M))}
  elif isinstance(M, dict): tags = {i:'' for i in M}
  elif isinstance(M, nx.classes.graph.Graph): tags = {i:'' for i in M.nodes()}
  n = len(tags)
  check = colNames if (isinstance(M, dict) and colNames) else (sorted(M[M.keys()[0]].keys()) if isinstance(M, dict) else sorted(tags.keys()))
  for col in chosen: check.remove(col)
  check = list(np.random.permutation(check)) if randOrder else sorted(check)
  seq = [] #####
  while len(distr) < n and len(chosen) < n:
    ####H to _
    (distr, tags, H, chosen) = pickColumn(M, tags, check, chosen=chosen, dictDefault=dictDefault, procs=procs)
    check.remove(chosen[-1])
    seq.append((chosen[-1], H))####
    if name: saveState(tags, chosen, progressFile, overwrite=True)
  if len(distr) < n and len(chosen) == n: return ([], []) ####'NO SOLUTION EXISTS'
  return (chosen, seq) #######chosen

#Determine the unchosen column maximizing change in entropy
#input: M - the object on which to perform multilateration
#       tags - tages given the columns already chosen
#       check - a list of columns left to check
#       chosen - optional list of already chosen columns, defaults to empty
#       dictDefault - optional default value to use if M is a dictionary of dictionaries, defaults to -1
#       procs - optional argument specifying the number of processes to use, defaults to 1
#return: the tag distribution with the new column, the new tags, the entropy with the new column, an updated list of chosen columns
def pickColumn(M, tags, check, chosen=[], dictDefault=-1, procs=1):
  (eMax, cMax, distr) = (-1, -1, {})
  if procs > 1:
    pool = mp.Pool(processes=procs)
    results = pool.map_async(colEntropy, [(col, M, tags, dictDefault) for col in check])
    results = results.get()
    pool.close()
    pool.join()
    (eMax, cMax, distr) = max(results)
  else:
    for col in check:
      (e, col, condDistr) = colEntropy((col, M, tags, dictDefault))
      if e > eMax: (eMax, cMax, distr) = (e, col, condDistr)
  chosen.append(cMax)
  if isinstance(M, list):
    for t in tags: tags[t] += ';'+str(M[t][cMax])
  elif isinstance(M, dict):
    for t in tags: tags[t] += ';'+str(M[t].get(cMax, dictDefault))
  elif isinstance(M, nx.classes.graph.Graph):
    Dcol = nx.single_source_shortest_path_length(M, cMax)
    for t in tags: tags[t] += ';'+str(Dcol.get(t, -1))
  return (distr, tags, eMax, chosen)

#Determine the joint entropy resulting from adding a given column
#Input is given as a single tuple so map_async may be used from pickColumn
#input: col - the column to add
#       M - the object on which to perform multilateration
#       tags - tags given the columns already chosen
#       dictDefault - default value to use if M is a dictionary of dictionaries
#return: the joint entropy, the column, and the distribution of tags
def colEntropy(args):
  (col, M, tags, dictDefault) = args
  getSymbol = lambda r,c: ''
  Dcol = {}
  if isinstance(M, list): getSymbol = lambda r,c: M[r][c]
  elif isinstance(M, dict): getSymbol = lambda r,c: M[r].get(c, dictDefault)
  elif isinstance(M, nx.classes.graph.Graph):
    Dcol = nx.single_source_shortest_path_length(M, col)
    getSymbol = lambda r,c: Dcol.get(r, -1)
  jointDistr = {}
  for elem in tags:
    t = tags[elem]+';'+str(getSymbol(elem, col))
    if t not in jointDistr: jointDistr[t] = 0
    jointDistr[t] += 1
  e = entropy(list(jointDistr.values()), base=2)
  return (e, col, jointDistr)

######################
### HAMMING GRAPHS ###
######################
#Computes the hamming distance or number of mismatches between two strings
#If one string is longer the other, only its prefix is used
#input: a, b - two sequences to compare
#return: the hamming distance between a and b
def hammingDist(a, b):
  return sum(1 for (x,y) in zip(a,b) if x!=y)

#Given a resolving set of a Hamming graph H(k, a), determine a resolving set for H(k+1, a) (see [2])
#input: resSet - a resolving set for H(k, a)
#       alphabet - the alphabet from which to draw characters for the new resolving set
#       rand - optional boolean, if true randomize resSet and alphabet order, default is false
#return: a resolving set for H(k+1, a)
def hammingConstruction(resSet, alphabet, rand=False):
  alphabet = [[a] for a in alphabet]
  if len(resSet)==0: return alphabet[:-1]
  if rand:
    resSet = map(list, np.random.permutation(resSet))
    alphabet = map(list, np.random.permutation(alphabet))
  newResSet = [r+alphabet[2*i] if 2*i<len(alphabet) else r+alphabet[0] for i,r in enumerate(resSet)]
  num = len(alphabet) / 2
  for i in range(num):
    v = resSet[i]+alphabet[2*i+1]
    newResSet.append(v)
  return newResSet

#Find all resolving sets of a Hamming graph via a brute force search for a particular size
#This may be extremely slow even for small values of k and alphabet
#input: k - the length of strings in the hamming graph
#       alphabet - the alphabet to use in the hamming graph
#       size - the size of sets to check
#       procs - the number of processes to use, default is 1
#       verbose - optional bool. if true and procs=1, print percent of sets checked every 10000 sets, default is false
#return: all resolving sets of the given size for the specified hamming graph
def hammingAllResolving(k, alphabet, size, procs=1, verbose=False):
  if isinstance(alphabet, str): alphabet = list(alphabet)
  resSets = []
  kmers = product(alphabet, repeat=k)
  if procs>1:
    pool = mp.Pool(processes=procs)
    results = pool.map_async(checkResolvingHammingTuple, zip(combinations(kmers, size), repeat(k), repeat(alphabet)))
    results = results.get()
    pool.close()
    pool.join()
    for (isResolving, R) in results:
      if isResolving: resSets.append(R)
  else:
    numCombos = comb(int(np.power(len(alphabet), k)), size)
    for i,R in enumerate(combinations(kmers, size)):
      if verbose and i%10000==0: print('Brute force progress: ', i / numCombos)
      if checkResolvingHamming(R, k, alphabet, verbose=verbose): resSets.append(R)
  return resSets

#A helper function accepting a tuple of arguments for checkResolvingHamming
#This allows pool.map_async to be used for multiprocessing
#input: a tuple containing 3 elements
#       R - a set of strings to check as resolving
#       k - length of strings
#       alphabet - characters that strings are composed of
#return: immediately calls checkResolvingHamming and returns the result in addition to the given set R
def checkResolvingHammingTuple(args):
  (R, k, alphabet) = args
  return (checkResolvingHamming(R, k, alphabet), R)

#Check that a given set of strings is resolving for a specified Hamming graph
#This may be extremely slow even for small values of k and alphabet
#input: R - a set of strings to check as resolving
#       k - length of strings
#       alphabet - characters that strings are composed of
#       procs - optional number of processes to use, default is 1
#       chunkSize - optional number of strings to check at once if procs>1, default is 1000
#       verbose - optional bool. if true, print percent of strings checked every 10000 sets or every chunkSize if procs>1, default is false
  if isinstance(alphabet, str): alphabet = list(alphabet)
  tags = {}
  if procs>1:
    kmers = zip(product(alphabet, repeat=k), repeat(R))
    chunk = list(islice(kmers, 0, chunkSize))
    chunkNum = 0
    while len(chunk)>0:
      if verbose:
        print('Check resolving Hamming progress: chunk', chunkNum)
        chunkNum += 1
      pool = mp.Pool(processes=procs)
      results = pool.map_async(genTag, chunk)
      results = results.get()
      pool.close()
      pool.join()
      for tag in results:
        if tag in tags: return False
        tags[tag] = 1
      chunk = list(islice(kmers, 0, chunkSize))
  else:
    tot = float(np.power(len(alphabet), k))
    for i,seq in enumerate(product(alphabet, repeat=k)):
      if verbose and i%10000==0: print('Check resolving Hamming progress: ', i/tot)
      tag = ';'.join(map(str, [hammingDist(list(seq), r) for r in R]))
      if tag in tags: return False
      tags[tag] = 1
  return True

#Argument is given as a tuple to allow use by map_async
#input: a tuple containing seq and R
#       seq - a sequence of interest
#       R - a set of sequences to determine the distance from to kmer
#return: a string containing the Hamming distances from seq to all elements of R separated by ;
def genTag(args):
  (seq, R) = args
  return ';'.join(map(str, [hammingDist(list(seq), r) for r in R]))

###########################
### CHECK RESOLVABILITY ###
###########################
#Given a set of columns and an object on which to check resolvability, check that the set is resolving
#input: R - a set of columns
#       M - an object on which to check the resolvability of R
#       colNames - optional list of columns. if M is a dictionary of dictionaries and colNames is not empty it will be used as the set of columns to consider, defaults to empty
#       dictDefault - optional default value to use if M is a dictionary of dictionaries, defaults to -1
#return: true if R is resolving and false otherwise
def checkResolving(R, M, colNames=[], dictDefault=-1):
  tags = {}
  elements = []
  if isinstance(M, list): elements = range(len(M))
  elif isinstance(M, dict): elements = M.keys()
  elif isinstance(M, nx.classes.graph.Graph): elements = M.nodes()
  for elem in elements:
    tag = []
    if isinstance(M, list): tag = [M[elem][r] for r in R]
    elif isinstance(M, dict): tag = [M[elem].get(r, dictDefault) for r in R]
    elif isinstance(M, nx.classes.graph.Graph): #tag = [nx.shortest_path_length(M, source=elem, target=r) for r in R]
      for r in R:
        try:
          tag.append(nx.shortest_path_length(M, source=elem, target=r))
        except:
          tag.append(-1)
    tag = ';'.join(map(str, tag))
    if tag in tags: return False
    tags[tag] = 1
  return True

#@title Graph Info (Hidden) { display-mode: "form" }
# Produce Graph Information

def graph_maker(n = 50, m = 25, m0 = 25):
  # Creates a Barabasi-Albert Graph with parameters
  # n = number of node
  # m = number of connections for each new node
  # m0 = number of starting nodes
  G = nx.Graph()
  GInit = nx.complete_graph(m0)
  G = nx.barabasi_albert_graph(n, m, initial_graph=GInit)
  return G

def data_maker(G, procs = 1):
  # Finds resolving sets and node entropies for graph G
  D = dict(nx.all_pairs_shortest_path_length(G))
  M = list(np.zeros((len(G), len(G)), dtype=int))
  diameter = -1
  distances = []
  for u in range(len(G)-1):
    for v in range(u+1, len(G)):
      M[u][v] = D[u].get(v, -1)
      M[v][u] = D[u].get(v, -1)
      if M[u][v] > diameter: diameter = M[u][v]
      if M[u][v] != -1: distances.append(M[u][v])
  nodeEntropies = {}
  for v in range(len(M[0])):
    distr = {}
    for r in range(len(M)):
      distr[M[r][v]] = distr.get(M[r][v], 0) + 1
    nodeEntropies[v] = entropy(list(distr.values()), base=2)
  (resSet, seq) = ich(M, procs=procs)
  return (resSet, nodeEntropies, M)

def classifier(indecies, noise = False):
  # Creates a 50x50 matrix classifying each column as part of the
  # resolving set or not
  # noise jitters the values to mimic real network outputs
  zeros = np.zeros((50, 50))
  for i in indecies:
    zeros[:,i] = 1
  # Introduce Noise for Classifier Model
  if noise:
    for i in range(0, 50):
      for j in range(0, 50):
        if zeros[i, j] == 1:
          if np.random.uniform(0, 1) < .8:
            zeros[i, j] = np.random.uniform(low = 0.5, high = 1.0)
          else:
            zeros[i, j] = np.random.uniform(low = 0.4, high = 1.0)
        else:
          if np.random.uniform(0, 1) < .8:
            zeros[i, j] = np.random.uniform(low = 0.0, high = 0.5)
          else:
            zeros[i, j] = np.random.uniform(low = 0.0, high = 0.6)
  return np.reshape(zeros, (50, 50, 1))

def diagonal_matrix(entropies):
  # Creates a diagonal matrix of the entropies
  zeros = np.zeros((50, 50))
  for i in range(50):
    zeros[i, i] = entropies[i]
  return zeros

def random_graph_generator(n = 50, m0 = 25, m = 25, count = 1000, noise = False):
  # Creates count number of random Barabasi-Albert graphs
  input = []
  output = []
  for i in range(count):
    m0 = np.random.randint(2, m0+1)
    m = np.random.randint(1, min(m, m0)+1)
    resSet, nodeEntropies, M = data_maker(graph_maker(n, m, m0))
    output.append(classifier(np.asarray(resSet), noise))
    nodeEntropies = diagonal_matrix(nodeEntropies)
    M = np.reshape(M, (50, 50, 1))
    nodeEntropies = np.reshape(nodeEntropies, (50, 50, 1))
    input.append(np.concatenate((M, nodeEntropies), axis = -1))
  return input, output

#@title Convolution Blocks (Hidden) { display-mode: "form" }
class ConvBlock(tf.keras.layers.Layer):
  def __init__(self, n_filters = 32, filter_size = 3, max_pooling = True):
    super(ConvBlock, self).__init__()
    self.conv1 = Conv2D(n_filters, # Number of filters
                  filter_size,   # Kernel size
                  activation='relu',
                  padding='same',
                  kernel_initializer='he_normal')
    self.conv2 = Conv2D(n_filters, # Number of filters
                  filter_size,   # Kernel size
                  activation='relu',
                  padding='same',
                  kernel_initializer='he_normal')
    self.conv3 = Conv2D(n_filters, # Number of filters
                  filter_size,   # Kernel size
                  activation='relu',
                  padding='same',
                  kernel_initializer='he_normal')
    self.pool = max_pooling
    self.maxpool = MaxPooling2D((2,2))

  def call(self, inputs):
    conv = self.conv1(inputs)
    conv = self.conv2(conv)
    conv = self.conv3(conv)

    if self.pool:
      next_layer = self.maxpool(conv)
    else:
      next_layer = conv

    skip_connection = conv

    return next_layer, skip_connection

class UpsamplingBlock(tf.keras.layers.Layer):
  def __init__(self, n_filters = 32, filter_size = 3, same_padding = True):
    super(UpsamplingBlock, self).__init__()
    if same_padding:
      self.up = Conv2DTranspose(
                  n_filters,    # number of filters
                  filter_size,    # Kernel size
                  strides=(2,2),
                  padding='same')
    else:
      self.up = Conv2DTranspose(
                  n_filters,    # number of filters
                  filter_size,    # Kernel size
                  strides=(2,2),
                  padding='valid')
    self.conv1 = Conv2D(n_filters,   # Number of filters
                 filter_size,     # Kernel size
                 activation='relu',
                 padding='same',
                 kernel_initializer='he_normal')
    self.conv2 = Conv2D(n_filters,  # Number of filters
                 filter_size,   # Kernel size
                 activation='relu',
                 padding='same',
                 kernel_initializer='he_normal')
    self.conv3 = Conv2D(n_filters,  # Number of filters
                 filter_size,   # Kernel size
                 activation='relu',
                 padding='same',
                 kernel_initializer='he_normal')
    
  def call(self, expansive_input, contractive_input):
    conv = self.up(expansive_input)
    merge = concatenate([conv, contractive_input], axis = 3)
    conv = self.conv1(merge)
    conv = self.conv2(conv)
    conv = self.conv3(conv)

    return conv

#@title UNet Model (Hidden) { display-mode: "form" }
class UNetModel(tf.keras.layers.Layer):
  def __init__(self):
    super(UNetModel, self).__init__()
    self.cblock1 = ConvBlock()
    self.cblock2 = ConvBlock(n_filters = 2*32)
    self.cblock3 = ConvBlock(n_filters = 4*32)
    self.cblock4 = ConvBlock(n_filters = 8*32)
    self.cblock5 = ConvBlock(n_filters = 16*32, max_pooling = False)

    self.ublock1 = UpsamplingBlock(n_filters = 8*32)
    self.ublock2 = UpsamplingBlock(n_filters = 4*32)
    self.ublock3 = UpsamplingBlock(n_filters = 2*32, same_padding = False)
    self.ublock4 = UpsamplingBlock()

    self.conv1 = Conv2D(32,
                    3,
                    activation='relu',
                    padding='same',
                    kernel_initializer='he_normal')
    self.conv2 = Conv2D(1, 1, padding='same')

  def call(self, inputs):
    cblock1 = self.cblock1(inputs)
    cblock2 = self.cblock2(cblock1[0])
    cblock3 = self.cblock3(cblock2[0])
    cblock4 = self.cblock4(cblock3[0])
    cblock5 = self.cblock5(cblock4[0])

    x = self.ublock1(cblock5[0], cblock4[1])
    x = self.ublock2(x, cblock3[1])
    x = self.ublock3(x, cblock2[1])
    x = self.ublock4(x, cblock1[1])

    x = self.conv1(x)
    x = self.conv2(x)

    return x

#@title Classifier Net (Hidden) { display-mode: "form" }
class MiniBlock(tf.keras.layers.Layer):
  def __init__(self, n_filters = 32, filter_size = 3):
    super(MiniBlock, self).__init__()
    self.conv1 = Conv2D(n_filters, filter_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')
    self.conv2 = Conv2D(n_filters, filter_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')
    self.conv3 = Conv2D(n_filters, filter_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')
    self.pool = MaxPooling2D((2,2))

  def call(self, inputs):
    x = self.conv1(inputs)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.pool(x)

    return x

class ClassifierModel(tf.keras.layers.Layer):
  def __init__(self):
    super(ClassifierModel, self).__init__()
    self.mini1 = MiniBlock()
    self.mini2 = MiniBlock(2*32)
    self.mini3 = MiniBlock(4*32)
    self.mini4 = MiniBlock(8*32)

    self.flat = Flatten()
    self.dense1 = Dense(1000)
    self.dense2 = Dense(500)
    self.dense3 = Dense(250)
    self.dense4 = Dense(100)
    self.dense5 = Dense(50)
    self.dense6 = Dense(25)
    self.dense7 = Dense(10)
    self.dense8 = Dense(1)

  def call(self, inputs):
    x = self.mini1(inputs)
    x = self.mini2(x)
    x = self.mini3(x)
    x = self.mini4(x)

    x = self.flat(x)
    x = self.dense1(x)
    x = self.dense2(x)
    x = self.dense3(x)
    x = self.dense4(x)
    x = self.dense5(x)
    x = self.dense6(x)
    x = self.dense7(x)
    x = self.dense8(x)

    return x

def make_finder():
  input = Input((50, 50, 2))
  out = UNetModel()(input)
  return tf.keras.Model(inputs = input, outputs = out)

def make_classifier():
  input = Input((50, 50, 3))
  out = ClassifierModel()(input)
  return tf.keras.Model(inputs = input, outputs = out)

finder = make_finder()
classifier_model = make_classifier()
classifier_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam())

def complete_model(finder, classifier):
  input = Input((50,50,2))
  out1 = finder(input)
  x = concatenate([out1, input])
  out2 = classifier(x)

  return tf.keras.Model(inputs = input, outputs = [out1, out2])

model = complete_model(finder, classifier_model)
model.compile(optimizer = tf.keras.optimizers.Adam())

from tensorflow.python.ops.gen_array_ops import concat

loss_object1 = tf.keras.losses.BinaryCrossentropy(from_logits = True)
loss_object2 = tf.keras.losses.BinaryCrossentropy(from_logits = True)
optimizer = tf.keras.optimizers.Adam()

def grad(model, inputs, target1, target2):
  with tf.GradientTape() as tape:
    out1, out2 = model(inputs)
    loss1 = loss_object1(target1, out1)
    loss = loss1 + 50*loss_object2(target2, out2)
  return loss1, tape.gradient(loss, model.trainable_variables)

def train_complete_model(model, n_epochs, n_graphs):

  input, finder, concat, classifier = model.layers

  for epoch in range(n_epochs):
    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.BinaryAccuracy()

    print("Generating Data for Epoch {:03d}".format(epoch + 1), end = "")

    # Generating Data for Classifier
    classifier_graphs, classifier_ressets = random_graph_generator(count = n_graphs, noise = True) # Classifer has noise added to prevent the expectation of perfect 1s and 0s
    classifier_graphs = tf.convert_to_tensor(classifier_graphs, dtype = tf.float32)
    classifier_ressets = tf.convert_to_tensor(classifier_ressets, dtype = tf.float32)
    classifier_input = tf.concat([classifier_graphs, classifier_ressets], axis = -1)

    # Generating Data for Full Model
    finder_graphs, finder_ressets = random_graph_generator(count = n_graphs)
    finder_graphs = tf.convert_to_tensor(finder_graphs, dtype = tf.float32)
    finder_ressets = tf.convert_to_tensor(finder_ressets, dtype = tf.float32)
    finder_joined = tf.concat([finder_graphs, finder(finder_graphs)], axis = -1)

    # Classifier Dataset
    mixed_graphs = tf.concat([classifier_input, finder_joined], axis = 0)

    classifier_labels = tf.constant([[1.]] * n_graphs + [[0.]] * n_graphs)

    dataset1 = tf.data.Dataset.from_tensor_slices((mixed_graphs, classifier_labels))
    dataset1 = dataset1.batch(2 * n_graphs // 10)
    
    # Full Model Dataset
    discriminator_labels = tf.constant([[1.]] * n_graphs)

    dataset2 = tf.data.Dataset.from_tensor_slices((finder_graphs, {'output_1': finder_ressets, 'output_2': discriminator_labels}))
    dataset2 = dataset2.batch(n_graphs // 10)

    # Train Classifier
    classifier.trainable = True

    for x, y in dataset1:
      classifier.train_on_batch(x, y)

    # Train Finder
    classifier.trainable = False

    for x, y in dataset2:
      y1, y2 = y['output_1'], y['output_2']
      loss, grads = grad(model, x, y1, y2)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))

      epoch_loss_avg.update_state(loss)  # Add current batch loss
      epoch_accuracy.update_state(y1, finder(x, training = True))

    print("\rEpoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch + 1,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))

train_complete_model(model, 50, 1000)
